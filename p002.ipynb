{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch as tch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib import pyplot\n",
    "\n",
    "samples = 5000\n",
    "#Let's divide the toy dataset into training (80%) and rest for validation.\n",
    "train_split = int(samples*0.8)\n",
    "#Create a dummy classification dataset\n",
    "X, y = make_blobs(n_samples=samples, centers=2, n_features=64,\n",
    "cluster_std=10, random_state=2020)\n",
    "y = y.reshape(-1,1)\n",
    "#Convert the numpy datasets to Torch Tensors\n",
    "X,y = tch.from_numpy(X),tch.from_numpy(y)\n",
    "X,y =X.float(),y.float()\n",
    "#Split the datasets inot train and test(validation)\n",
    "X_train, x_test = X[:train_split], X[train_split:]\n",
    "Y_train, y_test = y[:train_split], y[train_split:]\n",
    "#Print shapes of each dataset\n",
    "print(\"X_train.shape:\",X_train.shape)\n",
    "print(\"x_test.shape:\",x_test.shape)\n",
    "print(\"Y_train.shape:\",Y_train.shape)\n",
    "print(\"y_test.shape:\",y_test.shape)\n",
    "print(\"X.dtype\",X.dtype)\n",
    "print(\"y.dtype\",y.dtype)\n",
    "\n",
    "#Define a neural network with 3 hidden layers and 1 output layer\n",
    "#Hidden Layers will have 64,256 and 1024 neurons\n",
    "#Output layers will have 1 neuron\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        tch.manual_seed(2020)\n",
    "        self.fc1 = nn.Linear(64, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 1024)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.out = nn.Linear(1024, 1)\n",
    "        self.final = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        op = self.fc1(x)\n",
    "        op = self.relu1(op)\n",
    "        op = self.fc2(op)\n",
    "        op = self.relu2(op)\n",
    "        op = self.out(op)\n",
    "        y = self.final(op)\n",
    "        return y\n",
    "\n",
    "#Define function for training a network\n",
    "def train_network(model,optimizer,loss_function \\\n",
    ",num_epochs,batch_size,X_train,Y_train):\n",
    "#Explicitly start model training\n",
    "    model.train()\n",
    "    loss_across_epochs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss= 0.0\n",
    "        for i in range(0,X_train.shape[0],batch_size):\n",
    "            #Extract train batch from X and Y\n",
    "            input_data = X_train[i:min(X_train.shape[0],i+batch_size)] \n",
    "            labels = Y_train[i:min(X_train.shape[0],i+batch_size)]\n",
    "            #set the gradients to zero before starting to do backpropragation\n",
    "            optimizer.zero_grad()\n",
    "            #Forward pass\n",
    "            output_data = model(input_data)\n",
    "\n",
    "            #Caculate loss\n",
    "            loss = loss_function(output_data, labels)\n",
    "            #Backpropogate\n",
    "            loss.backward()\n",
    "            #Update weights\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_size\n",
    "    print(\"Epoch: {} - Loss:{:.4f}\".format(epoch+1,train_loss ))\n",
    "    loss_across_epochs.extend([train_loss])\n",
    "    #Predict\n",
    "    y_test_pred = model(x_test)\n",
    "    a =np.where(y_test_pred>0.5,1,0)\n",
    "    return(loss_across_epochs)\n",
    "    ###------------END OF FUNCTION--------------\n",
    "#Create an object of the Neural Network class\n",
    "model = NeuralNetwork()\n",
    "#Define loss function\n",
    "loss_function = nn.BCELoss() #Binary Crosss Entropy Loss\n",
    "#Define Optimizer\n",
    "adam_optimizer = tch.optim.Adam(model.parameters(),lr= 0.001)\n",
    "#Define epochs and batch size\n",
    "num_epochs = 10\n",
    "batch_size=16\n",
    "#Calling the function for training and pass model, optimizer, loss and related paramters\n",
    "adam_loss = train_network(model,adam_optimizer \\\n",
    ",loss_function,num_epochs,batch_size,X_train,Y_train)\n",
    "#Define loss function\n",
    "loss_function = nn.BCELoss() #Binary Crosss Entropy Loss\n",
    "num_epochs = 10\n",
    "batch_size=16\n",
    "#Define a model object from the class defined earlier\n",
    "model = NeuralNetwork()\n",
    "#Train network using RMSProp optimizer\n",
    "rmsprp_optimizer = tch.optim.RMSprop(model.parameters() , lr=0.01, alpha=0.9 , eps=1e-08, weight_decay=0.1 , momentum=0.1, centered=True)\n",
    "print(\"RMSProp...\")\n",
    "rmsprop_loss = train_network(model,rmsprp_optimizer,loss_function ,num_epochs,batch_size,X_train,Y_train)\n",
    "#Train network using Adam optimizer\n",
    "model = NeuralNetwork()\n",
    "adam_optimizer = tch.optim.Adam(model.parameters(),lr= 0.001)\n",
    "print(\"Adam...\")\n",
    "adam_loss = train_network(model,adam_optimizer,loss_function ,num_epochs,batch_size,X_train,Y_train)\n",
    "#Train network using SGD optimizer\n",
    "model = NeuralNetwork()\n",
    "sgd_optimizer = tch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "print(\"SGD...\")\n",
    "sgd_loss = train_network(model,sgd_optimizer,loss_function ,num_epochs,batch_size,X_train,Y_train)\n",
    "#Plot the losses for each optimizer across epochs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "epochs = range(0,10)\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(adam_loss,label=\"ADAM\")\n",
    "ax.plot(sgd_loss,label=\"SGD\")\n",
    "ax.plot(rmsprop_loss,label=\"RMSProp\")\n",
    "ax.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Overall Loss\")\n",
    "plt.title(\"Loss across epochs for different optimizers\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
